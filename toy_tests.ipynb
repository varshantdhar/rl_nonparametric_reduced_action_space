{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimComplete(Exception):\n",
    "    pass\n",
    "\n",
    "class ShuffleBoardEnv:\n",
    "    def __init__(self, device, goal_start_pos=10, goal_end_pos=20, coeff_friction=0.2, max_energy=100,\n",
    "                 block_mass_kg=1, gravity=9.8, action_reward=-1, goal_reward=0, max_steps=100):\n",
    "        \"\"\"\n",
    "        The agent in this toy example applies a force on an object every time step, \n",
    "        \n",
    "        Agent starts at position 0. In each state, the agent's action is a scalar energy applied in range [-1, 1],\n",
    "        which is a linear proportion of max_energy (e.g. an action of 0.5 applies max_energy/2).\n",
    "        (here we used a signed energy to allow movement in either direction).\n",
    "        Displacement is calculated as dx=applied_energy / (mass*gravity*coeff_friction)\n",
    "        \n",
    "        For each action, the agent receives a reward action_reward, unless the action moves the agent into the\n",
    "        goal region, which results in the agent receiving reward goal_reward and play restarts from x=0. If the\n",
    "        agent goes beyond end_pos, play also restarts.\n",
    "        \n",
    "        After taking a step, the agent receives updated state information as its current position.\n",
    "        \n",
    "        Parameters:\n",
    "        device: cpu or cuda\n",
    "        goal_start_pos: coordinate where goal region begins\n",
    "        goal_end_pos: coordinate where goal region ends\n",
    "        coeff_friction: friction coefficient for surface\n",
    "        max_energy: max energy the agent can apply in each step, in joules\n",
    "        block_mass_kg: mass of the block being push, in kilograms\n",
    "        gravity: m/s^2 gravity coefficient in the sim\n",
    "        action_reward: reward given to agent for each action that does not result in agent inside goal region\n",
    "        goal_reward: reward given to agent upon reaching goal region.\n",
    "        max_steps: restart episode after this many sim steps\n",
    "        \"\"\"\n",
    "        \n",
    "        self.goal_start_pos = goal_start_pos\n",
    "        self.goal_end_pos = goal_end_pos\n",
    "        self.coeff_friction = coeff_friction\n",
    "        self.max_energy = torch.Tensor([max_energy]).to(device)\n",
    "        self.block_mass_kg = block_mass_kg\n",
    "        self.gravity = gravity\n",
    "        self.action_reward = action_reward\n",
    "        self.goal_reward = goal_reward\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        # absolute movement limit, aka state_scaling in Q_Learning class\n",
    "        self.boundary = 1000\n",
    "        \n",
    "        self.device = device\n",
    "        self.initialized = False\n",
    "        \n",
    "        \n",
    "    def reset(self, batch_size):\n",
    "        \"\"\"\n",
    "        batch_size: how many simulations to run simultaneously with these sim parameters\n",
    "        \"\"\"\n",
    "        self.positions = torch.zeros(batch_size).to(device)\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "        self.done_flags = (torch.zeros(self.batch_size).to(device)) == 1\n",
    "        self.initialized = True\n",
    "        \n",
    "    def step(self, input_energy):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - rewards: batch_size vector of rewards after this step\n",
    "        - done_flags: batch_size bool vector, true if that sim is completed\n",
    "        \"\"\"\n",
    "        if not self.initialized or torch.all(self.done_flags):\n",
    "            raise SimComplete()\n",
    "        applied_energy = input_energy * self.max_energy\n",
    "        displacement = applied_energy/(self.block_mass_kg * self.gravity * self.coeff_friction)\n",
    "        self.positions += displacement\n",
    "        \n",
    "        reached_goal = (self.positions > self.goal_start_pos) * (self.positions < self.goal_end_pos)\n",
    "        rewards = torch.zeros(self.batch_size)\n",
    "        \n",
    "        rewards[~reached_goal] += self.action_reward\n",
    "        rewards[reached_goal] += self.goal_reward\n",
    "        rewards[self.done_flags] = 0\n",
    "            \n",
    "        self.steps += 1\n",
    "        if self.steps > self.max_steps or torch.any(torch.abs(self.positions) > self.boundary):\n",
    "            all_done_flag = torch.ones(self.batch_size).to(device) == 1\n",
    "            return rewards, all_done_flag\n",
    "        self.done_flags += reached_goal\n",
    "        return rewards, self.done_flags\n",
    "        \n",
    "    def observe_state(self):\n",
    "        return self.positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAValueNN(nn.Module):\n",
    "    def __init__(self, num_hidden, num_actions):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        simple shallow ReLU network:\n",
    "        \n",
    "        \"\"\"\n",
    "        self.model = nn.Sequential(nn.Linear(1, num_hidden),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(num_hidden, num_actions))\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "class Q_Learning:\n",
    "    def __init__(self, epsilon, gamma, value_model, target_model, action_space, state_scaling=100):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.value_model = value_model\n",
    "        self.target_model = target_model\n",
    "        self.action_space = action_space\n",
    "        self.num_actions = sum(action_space.shape)\n",
    "        self.optimizer = torch.optim.Adam(self.value_model.parameters(), lr=0.01)\n",
    "        \n",
    "        self.state_scaling = state_scaling\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.value_model.state_dict())\n",
    "        \n",
    "    def random_sample(self):\n",
    "        # for now, assume action space is a vector of actions\n",
    "        return random.randrange(self.num_actions)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        End goal:\n",
    "        sample actions given a state and action space\n",
    "            action space might be a list of actions\n",
    "        how to define action space?\n",
    "            maybe easiest is a list of values (since this is a continuous action space)\n",
    "        \"\"\"\n",
    "        state = state/self.state_scaling\n",
    "        q_values = self.predict_values(state)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            rand_ind = self.random_sample()\n",
    "            return rand_ind, q_values\n",
    "        \n",
    "        # otherwise, compute value for each of these actions\n",
    "        best_value, best_action_ind = torch.max(q_values, dim=0)\n",
    "        return best_action_ind, q_values\n",
    "    \n",
    "    def calc_loss(self, q_values, action_taken_ind, target_values, reward, done_flags):\n",
    "        \"\"\"\n",
    "        q_value: predicted value for taken action\n",
    "        target_values: value for each action for update target\n",
    "        reward: for taken action\n",
    "        done_flag: whether to mask this update (TODO for batch q learning)\n",
    "        \"\"\"\n",
    "        max_target = torch.max(target_values)\n",
    "        q_target = reward + self.gamma*max_target\n",
    "        return torch.square(q_values[action_taken_ind] - q_target)\n",
    "    \n",
    "    def update(self, loss):\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def target_values(self, state):\n",
    "        state = state / self.state_scaling\n",
    "        return self.target_model(state)\n",
    "    \n",
    "    def predict_values(self, state):\n",
    "        state = state / self.state_scaling\n",
    "        return self.value_model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ShuffleBoardEnv(device, goal_start_pos=55, goal_end_pos=60, goal_reward=10)\n",
    "\n",
    "action_space = torch.Tensor([0.2*x+0.1 for x in range(-5, 5)])\n",
    "value_model_NN = SAValueNN(num_hidden=5, num_actions=action_space.numel())\n",
    "target_model_NN = SAValueNN(num_hidden=5, num_actions=action_space.numel())\n",
    "\n",
    "q_agent = Q_Learning(epsilon=0.1, gamma=0.99, value_model=value_model_NN,\n",
    "                    target_model = target_model_NN, action_space=action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop:\n",
    "nIters = 1000\n",
    "reward_list = []\n",
    "\n",
    "target_model_update_freq = 500\n",
    "\n",
    "for i in range(nIters):\n",
    "    env.reset(1)\n",
    "    episode_running = True\n",
    "    \n",
    "    state = env.observe_state()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    if i % target_model_update_freq == 0:\n",
    "        q_agent.update_target_model()\n",
    "    \n",
    "    while True:\n",
    "        q_agent.optimizer.zero_grad()\n",
    "        \n",
    "        action_ind, values = q_agent.get_action(state)\n",
    "        \n",
    "        reward, flag = env.step(action_space[action_ind])\n",
    "        episode_reward += reward.detach()\n",
    "        \n",
    "        new_state = env.observe_state()\n",
    "        \n",
    "        target_values = q_agent.target_values(new_state)\n",
    "        loss = q_agent.calc_loss(values, action_ind, target_values, reward, flag)\n",
    "        q_agent.update(loss)\n",
    "        if flag:\n",
    "            # episode over\n",
    "            #print(episode_reward)\n",
    "            reward_list.append(episode_reward)\n",
    "            break\n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run(agent, env, action_space):\n",
    "    env.reset(1)\n",
    "    store_eps = agent.epsilon\n",
    "    agent.epsilon = 0\n",
    "    \n",
    "    total_reward = 0\n",
    "    done_flag = False\n",
    "    while not done_flag:\n",
    "        action, values = agent.get_action(env.positions)\n",
    "        reward, done_flag = env.step(action_space[action])\n",
    "        total_reward += reward\n",
    "    agent.epsilon = store_eps\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_run(q_agent, env, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5812, -0.1380,  0.4887, -0.3369, -0.0779], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_agent.predict_values(env.positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25.5102])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 2.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1])\n",
    "b = torch.Tensor([1, 2])\n",
    "torch.cat((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([state[0], action_space[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackw\\Anaconda3\\envs\\ml_env\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.74741533e-03, -1.30260282e-05,  1.01329319e-03, -1.59999108e-02,\n",
       "        9.19707566e-02, -1.33720273e-03,  8.60263631e-01,  2.39109698e-03,\n",
       "        1.00000000e+00,  3.23770605e-02, -1.33711053e-03,  8.53812397e-01,\n",
       "        9.45682094e-04,  1.00000000e+00,  4.40814018e-01,  4.45820123e-01,\n",
       "        4.61422771e-01,  4.89550203e-01,  5.34102798e-01,  6.02461040e-01,\n",
       "        7.09148884e-01,  8.85931849e-01,  1.00000000e+00,  1.00000000e+00])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not done:\n",
    "    env.render()\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 96, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7907f662aaf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'done' is not defined"
     ]
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
